{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5087b0e",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc63c79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (225969832.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_760/225969832.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install gradio\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#gradio 설치\n",
    "# pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff87cac",
   "metadata": {},
   "source": [
    "## 시도 1. Gradio를 사용하여 이미지에 세피아 효과를 적용하는 인터페이스를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b02ea5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://89504f1b43285e8021.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://89504f1b43285e8021.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189], \n",
    "        [0.349, 0.686, 0.168], \n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\") #gr.Image(type=\"filepath\", shape=...)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85897fb8",
   "metadata": {},
   "source": [
    "## 시도 2. Gradio를 사용하여 이미지에 웜톤 필터를 적용하는 인터페이스를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "def tone_filter(input_img):\n",
    "    input_img = input_img.convert(\"RGB\")  # 이미지를 RGB로 변환\n",
    "    input_arr = np.array(input_img)  # PIL.Image 객체를 NumPy 배열로 변환\n",
    "    hsv_arr = np.array(input_img.convert(\"HSV\"))  # HSV로 변환한 NumPy 배열\n",
    "    hue = hsv_arr[:, :, 0]\n",
    "    is_warm_tone = np.logical_or(np.logical_and(hue > 0, hue < 60), np.logical_and(hue > 300, hue <= 360))\n",
    "    filtered_arr = np.zeros_like(input_arr)\n",
    "    filtered_arr[is_warm_tone] = input_arr[is_warm_tone]\n",
    "    filtered_img = Image.fromarray(filtered_arr)  # NumPy 배열을 PIL.Image 객체로 변환\n",
    "    return filtered_img\n",
    "\n",
    "iface = gr.Interface(fn=tone_filter, inputs=\"image\", outputs=\"image\")\n",
    "iface.launch(share='True')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b55712",
   "metadata": {},
   "source": [
    " - tone_filter 함수는 입력 이미지에 웜톤 필터를 적용하여 웜톤 이미지를 생성한다.\n",
    "\n",
    " - 주어진 이미지를 RGB로 변환하고, 이를 NumPy 배열로 변환한 후 HSV로 변환한다.\n",
    "\n",
    " - HSV 배열에서 색상(Hue) 채널을 추출하여 웜톤 여부를 확인할 수 있다.\n",
    " \n",
    " - 하지만, NumPy 배열 객체에는 'convert'라는 속성(attribute)이 없어서 발생하는 오류로 실행이 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cb29c",
   "metadata": {},
   "source": [
    "## 시도3. StyleTransferModel이라는 사용자 정의 스타일 변환 모델을 정의하고 수행\n",
    " - 이 모델은 사전 훈련된 VGG19 모델의 일부 레이어를 사용하여 스타일 변환을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "\n",
    "# 스타일 변환 모델 정의\n",
    "class StyleTransferModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__()\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        model = vgg19(pretrained=False)\n",
    "        state_dict = load_state_dict_from_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth', progress=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        return model.features[:6]\n",
    "\n",
    "    def forward(self, content, style):\n",
    "        content_features = self.model(content)\n",
    "        style_features = self.model(style)\n",
    "\n",
    "        return content_features, style_features\n",
    "\n",
    "# 스타일 변환 함수\n",
    "def style_transfer(content_image, style_image):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    content_image = Image.fromarray(content_image)\n",
    "    style_image = Image.fromarray(style_image)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    content_tensor = transform(content_image).unsqueeze(0).to(device)\n",
    "    style_tensor = transform(style_image).unsqueeze(0).to(device)\n",
    "\n",
    "    model = StyleTransferModel().to(device)\n",
    "    content_features, style_features = model(content_tensor, style_tensor)\n",
    "\n",
    "    # 여기에서 스타일 변환 작업을 수행하고 결과 이미지를 생성합니다.\n",
    "    # 예시로서 content_features와 style_features를 이용하여 다른 이미지를 생성하도록 작성해두었습니다.\n",
    "    # 실제로는 원하는 스타일 변환 알고리즘 또는 모델로 대체해야 합니다.\n",
    "    output_image = content_features + style_features  # 예시로서 content_features와 style_features를 더한 결과를 생성\n",
    "\n",
    "    output_image_np = output_image.squeeze(0).detach().cpu().numpy()  # NumPy 배열로 변환\n",
    "\n",
    "    # 결과 이미지를 PIL.Image 객체로 변환하여 반환합니다.\n",
    "    output_image_pil = Image.fromarray((output_image_np * 255).astype(np.uint8))\n",
    "\n",
    "    return output_image_pil\n",
    "\n",
    "iface = gr.Interface(fn=style_transfer, inputs=[\"image\", \"image\"], outputs=\"image\")\n",
    "iface.launch(share='True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb43096",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg19, vgg19_bn\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "\n",
    "def style_transfer(content_image, style_image):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 전처리: 이미지 크기 조정 및 텐서로 변환\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    content_tensor = transform(content_image).unsqueeze(0).to(device)\n",
    "    style_tensor = transform(style_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # 사전 훈련된 VGG19 모델 로드\n",
    "    model = vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "    # 스타일 변환 수행\n",
    "    with torch.no_grad():\n",
    "        content_features = model(content_tensor)\n",
    "        style_features = model(style_tensor)\n",
    "\n",
    "        # 여기에서 스타일 변환 작업을 수행하고 결과 이미지를 생성합니다.\n",
    "        # 예를 들어, content_features와 style_features를 이용하여 다른 이미지를 생성합니다.\n",
    "        output_features = content_features + style_features\n",
    "\n",
    "    # 후처리: 텐서를 이미지로 변환\n",
    "    output_image = output_features.squeeze(0).cpu().numpy()\n",
    "    output_image = output_image.transpose((1, 2, 0))\n",
    "    output_image = np.clip(output_image, 0, 1)  # 범위를 0과 1 사이로 클리핑\n",
    "    output_image = (output_image * 255).astype(np.uint8)\n",
    "\n",
    "    return output_image\n",
    "\n",
    "iface = gr.Interface(fn=style_transfer, inputs=[\"image\", \"image\"], outputs=\"image\")\n",
    "iface.launch(import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "\n",
    "# 스타일 변환 모델 정의\n",
    "class StyleTransferModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__()\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        model = vgg19(pretrained=False)\n",
    "        state_dict = load_state_dict_from_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth', progress=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        return model.features[:6]\n",
    "\n",
    "    def forward(self, content, style):\n",
    "        content_features = self.model(content)\n",
    "        style_features = self.model(style)\n",
    "\n",
    "        return content_features, style_features\n",
    "\n",
    "# 스타일 변환 함수\n",
    "def style_transfer(content_image, style_image):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    content_image = Image.fromarray(content_image)\n",
    "    style_image = Image.fromarray(style_image)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    content_tensor = transform(content_image).unsqueeze(0).to(device)\n",
    "    style_tensor = transform(style_image).unsqueeze(0).to(device)\n",
    "\n",
    "    model = StyleTransferModel().to(device)\n",
    "    content_features, style_features = model(content_tensor, style_tensor)\n",
    "\n",
    "    # 여기에서 스타일 변환 작업을 수행하고 결과 이미지를 생성합니다.\n",
    "    # 예시로서 content_features와 style_features를 이용하여 다른 이미지를 생성하도록 작성해두었습니다.\n",
    "    # 실제로는 원하는 스타일 변환 알고리즘 또는 모델로 대체해야 합니다.\n",
    "    output_image = content_features + style_features  # 예시로서 content_features와 style_features를 더한 결과를 생성\n",
    "\n",
    "    output_image_np = output_image.squeeze(0).detach().cpu().numpy()  # NumPy 배열로 변환\n",
    "\n",
    "    # 결과 이미지를 PIL.Image 객체로 변환하여 반환합니다.\n",
    "    output_image_pil = Image.fromarray((output_image_np * 255).astype(np.uint8))\n",
    "\n",
    "    return output_image_pil\n",
    "\n",
    "iface = gr.Interface(fn=style_transfer, inputs=[\"image\", \"image\"], outputs=\"image\")\n",
    "iface.launch(share='True')\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
